# -*- coding: utf-8 -*-
"""rnn_prefetcher_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16Uh5q2PrRTfeUFw_5bJPYhWEj9KfCa7p

##import
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
from keras.datasets import imdb
from keras.preprocessing import sequence
import keras
import tensorflow as tf
import os
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

"""##change hyperparameter to make best model"""

seq_length = 5 # length of sequence for a training example
epoch = 20
MAXLEN = 43
train_model_percentage = 80

cache_size = 1000

output_encoding_number = 5

BATCH_SIZE = 64
ADDRESS_PREDICT_SIZE = 1001
EMBEDDING_DIM = 64
RNN_UNITS = 1024

FIRST_OUTPUT_SIZE = 256
SECOND_OUTPUT_SIZE = 16
FINAL_HIDDEN_SIZE = 128

BUFFER_SIZE = 10000

"""##open text file"""

f = open("bfs-10.txt", 'r')

"""##text file split"""

unique_instr_id = list()
cycle_count = list()
load_address = list()
instr_pointer_load = list()
llc_hit_miss = list()

count = 0

f.seek(0, 0)
while True:
  line = f.readline()
  if not line: break
  split_line = line.split(', ')
  print(split_line[0])
  unique_instr_id.append(split_line[0])
  cycle_count.append(split_line[1])
  load_address.append(split_line[2])
  instr_pointer_load.append(split_line[3])
  llc_hit_miss.append(split_line[4][:-1])

for i in range(len(load_address)):
  load_address[i] = int(load_address[i], 16)//64

"""##make delta list(first input)"""

delta = list()
for i in range(len(load_address)-1):
  delta.append(load_address[i+1] - load_address[i])

plot_this = list()
for i in range(len(load_address)-1):
  if delta[i] > 2000:
    continue
  if delta[i] < -2000:
    continue
  plot_this.append(delta[i])
plt.hist(plot_this, bins=100)
plt.show()

for i in range(len(delta)):
  if delta[i] > 500:
    delta[i] = 1000
    continue
  if delta[i] < -500:
    delta[i] = 0
    continue
  delta[i] += 500

print(delta[0:100])
print(max(load_address))

delta_bundle = list()
for i in range(len(delta)-seq_length-1):
  delta_bundle.append(delta[i:i+seq_length+1])
delta_bundle = np.array(delta_bundle)
print(delta_bundle)

rnn_data = delta_bundle[:,:-1]
rnn_data = np.array(rnn_data)
print(rnn_data)

"""##make address binary input(second input)"""

# 8 -> [0,0,0,1] change int to categorical value
def address_to_binary(a):
  binary_int = list()
  for i in range(MAXLEN):
    # /1000 -> scaling
    binary_int.append(a%2)
    a = a//2
  return binary_int

print(address_to_binary(8))

address_binary_input = list()
for i in range(seq_length+1, len(load_address)-1):
  address_binary_input.append(address_to_binary(load_address[i]))

print(address_binary_input[0])
address_binary_input = np.array(address_binary_input)

"""##make output data"""

delta_output = delta_bundle[:,-1:]
delta_output = delta_output.flatten()
print(delta_output)

"""##kick dummy input and output"""

delete_list = list()
for i in range(len(delta_output)):
  if delta_output[i] == 0 or delta_output[i] == 1000:
    delete_list.append(i)

delta_output = np.delete(delta_output, delete_list)
rnn_data = np.delete(rnn_data, delete_list, 0)
address_binary_input = np.delete(address_binary_input, delete_list, 0)

print(delta_output)

"""##make train and test data"""

cut_index = len(rnn_data) * train_model_percentage // 100

train_rnn_data = rnn_data[:cut_index,:]
test_rnn_data = rnn_data[cut_index:]

train_address_binary_input = address_binary_input[:cut_index,:]
test_address_binary_input = address_binary_input[cut_index:,:]

train_delta_output = delta_output[:cut_index]
test_delta_output = delta_output[cut_index:]

"""##shuffle train and test data ( X )"""

idx = np.arange(train_rnn_data.shape[0])
np.random.shuffle(idx)

train_rnn_data = train_rnn_data[idx]
train_address_binary_input = train_address_binary_input[idx]
train_delta_output = train_delta_output[idx]

idx = np.arange(test_rnn_data.shape[0])
np.random.shuffle(idx)

test_rnn_data = test_rnn_data[idx]
test_address_binary_input = test_address_binary_input[idx]
test_delta_output = test_delta_output[idx]

"""##make model"""

from keras.layers import Input, Dense, Embedding, LSTM
from keras.models import Model

def build_model(address_predict_size, first_output_size, second_output_size, final_hidden_size, embedding_dim, rnn_units):
  
  input_x = Input(shape=(seq_length,))
  x = Embedding(address_predict_size, embedding_dim)(input_x)
  x = LSTM(rnn_units, recurrent_initializer='glorot_uniform') (x)
  x = Dense(first_output_size, activation="sigmoid") (x)
  x = Model(inputs=input_x, outputs=x)

  input_y = Input(shape=(MAXLEN,))
  y = Dense(second_output_size, activation="relu")(input_y)
  y = Model(inputs=input_y, outputs=y)

  combined = tf.keras.layers.concatenate([x.output, y.output])

  z = Dense(final_hidden_size, activation="relu")(combined)
  z = Dense(address_predict_size, activation="sigmoid")(z)

  model = Model(inputs=[x.input, y.input], outputs=z)

  return model

model = build_model(ADDRESS_PREDICT_SIZE, FIRST_OUTPUT_SIZE, SECOND_OUTPUT_SIZE, FINAL_HIDDEN_SIZE, EMBEDDING_DIM, RNN_UNITS)
model.summary()

from tensorflow.keras.utils import plot_model
plot_model(model, to_file='model.png')
plot_model(model, to_file='model_shapes.png', show_shapes=True)

"""##create loss function"""

def loss(labels, logits):
  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits)

model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])

# Directory where the checkpoints will be saved
checkpoint_dir = './training_checkpoints'
# Name of the checkpoint files
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}")

checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_prefix,
    save_weights_only=True)

history = model.fit([train_rnn_data, train_address_binary_input], train_delta_output, batch_size=BATCH_SIZE, epochs=epoch, callbacks=[checkpoint_callback])

plt.plot(history.history['loss'])
plt.title('Model loss graph')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()

plt.plot(history.history['accuracy'])
plt.title('train_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.show()

def generate_address(model, start_address):
  # Evaluation step (generating address using the learned model)

  # Number of characters to generate
  num_generate = 2

  input_eval = [tf.expand_dims(start_address[0], 0), tf.expand_dims(start_address[1], 0)]

  # Empty string to store our results
  generated = []

  # Low temperatures results in more predictable.
  temperature = 1.0

  # Here batch size == 1
  model.reset_states()
  
  predictions = model(input_eval)

  predictions = predictions / temperature

  # using probability
  '''
  generated = tf.random.categorical(predictions_np, num_samples=num_generate)[-1,0].numpy()  
  '''

  # using argmax
  predictions_np = predictions.numpy()
  predictions_np = np.squeeze(predictions_np,axis=0)
  predictions_np = np.argsort(predictions_np)
  generated = predictions_np [-num_generate:]

  return generated

"""##accuracy 측정"""

total_num = len(delta)
total_num_except_0_1000 = len(rnn_data)
correct = 0
first_correct = 0
second_correct = 0

cache = np.zeros(cache_size)

for i in tqdm(range(len(test_delta_output)), desc='check accuracy..'):
  inp = [test_rnn_data[i].tolist(), test_address_binary_input[i].tolist()]
  lstm_ans = generate_address(model, inp)
  if test_delta_output[i] in lstm_ans:
    correct += 1
    if test_delta_output[i] == lstm_ans[0]:
      first_correct += 1
    if test_delta_output[i] == lstm_ans[1]:
      second_correct += 1

print("\naccuracy")
print(correct/total_num)
print("\naccuracy except 0 and 1000")
print(correct/total_num_except_0_1000)
print("first number accuracy")
print(first_correct/total_num_except_0_1000)
print("second number accuracy")
print(second_correct/total_num_except_0_1000)

"""#seq_length = 5
#######################################
####epoch = 30, second_input_length = 43
####embedding_dimention = 64, batch_size = 64
#######################################
####epoch = 30, second_input_length = 43
####embedding_dimention = 256, batch_size = 64

#seq_length = 10

#seq_length = 20

#seq_length = 30

https://jaehyeongan.github.io/2019/03/26/KERAS-FUNCTIONAL-API-MULTI-INPUT-%EB%AA%A8%EB%8D%B8-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0/

https://gooopy.tistory.com/103?category=876252
"""