# -*- coding: utf-8 -*-
"""rnn_prefetcher_1, (2) split_slide, test_train_shuffle.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jUsQ-entc0NUHNGW8iaQHrCJJJqVf4sb

##import
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
from keras.datasets import imdb
from keras.preprocessing import sequence
import keras
import tensorflow as tf
import os
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

seq_length = 10 # length of sequence for a training example
epoch = 15
train_model_percentage = 80

f = open("473.astar-s0.txt", 'r')

unique_instr_id = list()
cycle_count = list()
load_address = list()
instr_pointer_load = list()
llc_hit_miss = list()

f.seek(0, 0)
while True:
  line = f.readline()
  if not line: break
  split_line = line.split(', ')
  unique_instr_id.append(split_line[0])
  cycle_count.append(split_line[1])
  load_address.append(split_line[2])
  instr_pointer_load.append(split_line[3])
  llc_hit_miss.append(split_line[4][:-1])

for i in range(len(load_address)):
  load_address[i] = int(int(load_address[i], 16)/64)

delta = list()
for i in range(len(load_address)-1):
  delta.append(load_address[i+1] - load_address[i])

plot_this = list()
for i in range(len(load_address)-1):
  if delta[i] > 2000:
    continue
  if delta[i] < -2000:
    continue
  plot_this.append(delta[i])
plt.hist(plot_this, bins=100)
plt.show()

for i in range(len(delta)):
  if delta[i] > 500:
    delta[i] = 1000
    continue
  if delta[i] < -500:
    delta[i] = 0
    continue
  delta[i] += 500

print(delta[0:100])
print(max(load_address))

delta_bundle = list()
for i in range(len(delta)-seq_length-1):
  delta_bundle.append(delta[i:i+seq_length+1])
delta_bundle = np.array(delta_bundle)
print(delta_bundle)

rnn_data = delta_bundle[:,:-1]
rnn_data = np.array(rnn_data)
print(rnn_data)

delta_output = delta_bundle[:,-1:]
delta_output = delta_output.flatten()
print(delta_output)

delete_list = list()
for i in range(len(delta_output)):
  if delta_output[i] == 0 or delta_output[i] == 1000:
    delete_list.append(i)

delta_output = np.delete(delta_output, delete_list)
rnn_data = np.delete(rnn_data, delete_list, 0)

print(delta_output)

cut_index = len(rnn_data) * train_model_percentage // 100

train_rnn_data = rnn_data[:cut_index,:]
test_rnn_data = rnn_data[cut_index:]

train_delta_output = delta_output[:cut_index]
test_delta_output = delta_output[cut_index:]

"""##make model"""

BATCH_SIZE = 64
ADDRESS_PREDICT_SIZE = 1001
EMBEDDING_DIM = 64
RNN_UNITS = 1024
BUFFER_SIZE = 10000

from keras.layers import Input, Dense, Embedding, LSTM
from keras.models import Model

def build_model(address_predict_size, embedding_dim, rnn_units):
  input_x = Input(shape=(seq_length,))
  x = Embedding(address_predict_size, embedding_dim)(input_x)
  x = LSTM(rnn_units, recurrent_initializer='glorot_uniform') (x)
  x = Dense(address_predict_size, activation="sigmoid") (x)
  model = Model(inputs=input_x, outputs=x)
  return model

model = build_model(ADDRESS_PREDICT_SIZE,EMBEDDING_DIM, RNN_UNITS)
model.summary()

"""##Create loss function"""

def loss(labels, logits):
  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits)

model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])

# Directory where the checkpoints will be saved
checkpoint_dir = './training_checkpoints'
# Name of the checkpoint files
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}")

checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_prefix,
    save_weights_only=True)

history = model.fit(train_rnn_data, train_delta_output, epochs=epoch, batch_size = BATCH_SIZE, callbacks=[checkpoint_callback])

def generate_address(model, start_address):
  # Evaluation step (generating address using the learned model)

  # Number of characters to generate
  num_generate = 2
  input_eval = tf.expand_dims(start_address, 0)

  # Empty string to store our results
  generated = []

  # Here batch size == 1
  model.reset_states()
  predictions = model(input_eval)

  # using argmax
  predictions_np = predictions.numpy()
  predictions_np = np.squeeze(predictions_np,axis=0)
  predictions_np = np.argsort(predictions_np)
  generated = predictions_np [-num_generate:]

  return generated

"""##accuracy 측정"""

total_num_except_0_1000 = 0
correct = 0
first_correct = 0
second_correct = 0

for i in tqdm(range(len(test_delta_output)), desc='check accuracy..'):
  total_num_except_0_1000 += 1
  inp = test_rnn_data[i]
  lstm_ans = generate_address(model, inp)
  if test_delta_output[i] in lstm_ans:
    correct += 1
    if test_delta_output[i] == lstm_ans[0]:
      first_correct += 1
    if test_delta_output[i] == lstm_ans[1]:
      second_correct += 1

print("\naccuracy except 0 and 1000")
print(correct/total_num_except_0_1000)
print("first number accuracy")
print(first_correct/total_num_except_0_1000)
print("second number accuracy")
print(second_correct/total_num_except_0_1000)

"""#seq_length = 10
####epoch =
####train_accuracy 
####test_accuracy 
####first number accuracy 
####second number accuracy 
####epoch = 20
####train_accuracy 86.80%
####test_accuracy 45.02%
####first number accuracy 7.04%
####second number accuracy 37.98%
####epoch = 30
####train_accuracy 87.38%
####test_accuracy 44.61%
####first number accuracy 6.89%
####second number accuracy 37.72%

https://jaehyeongan.github.io/2019/03/26/KERAS-FUNCTIONAL-API-MULTI-INPUT-%EB%AA%A8%EB%8D%B8-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0/

https://gooopy.tistory.com/103?category=876252
"""