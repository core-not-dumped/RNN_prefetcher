# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19XRFstPuEUGuPCTNvdW5zKUcuqwyeVIj

##import
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
from keras.datasets import imdb
from keras.preprocessing import sequence
import keras
import tensorflow as tf
import os
import numpy as np
import matplotlib.pyplot as plt

f = open("473.astar-s0.txt", 'r')

unique_instr_id = list()
cycle_count = list()
load_address = list()
instr_pointer_load = list()
llc_hit_miss = list()

f.seek(0, 0)
while True:
  line = f.readline()
  if not line: break
  split_line = line.split(', ')
  unique_instr_id.append(split_line[0])
  cycle_count.append(split_line[1])
  load_address.append(split_line[2])
  instr_pointer_load.append(split_line[3])
  llc_hit_miss.append(split_line[4][:-1])

for i in range(len(load_address)):
  load_address[i] = int(int(load_address[i], 16)/64)

delta = list()
for i in range(len(load_address)-1):
  delta.append(load_address[i+1] - load_address[i])

plot_this = list()
for i in range(len(load_address)-1):
  if delta[i] > 2000:
    continue
  if delta[i] < -2000:
    continue
  plot_this.append(delta[i])
plt.hist(plot_this, bins=100)
plt.show()

for i in range(len(delta)):
  if delta[i] > 500:
    delta[i] = 1000
    continue
  if delta[i] < -500:
    delta[i] = 0
    continue
  delta[i] += 500

print(delta[0:100])

"""##make set"""

seq_length = 5 # length of sequence for a training example
examples_per_epoch = len(delta)//(seq_length+1)
dataset = tf.data.Dataset.from_tensor_slices(delta)

sequences = dataset.batch(seq_length+1, drop_remainder=True)

def split_input_target(chunk):  # for the example: 1 2 3 4 5
    input_text = chunk[:-1]  # 1 2 3 4
    target_text = chunk[1:]  # 2 3 4 5
    return input_text, target_text  # 1 2 3 4, 2 3 4 5

dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry

for x, y in dataset.take(2):
  print("\n\nEXAMPLE\n")
  print("INPUT")
  print(x)
  print("\nOUTPUT")
  print(y)

BATCH_SIZE = 64
ADDRESS_PREDICT_SIZE = 1001  # vocab is number of unique characters
EMBEDDING_DIM = 256
RNN_UNITS = 1024

# Buffer size to shuffle the dataset
# (TF data is designed to work with possibly infinite sequences,
# so it doesn't attempt to shuffle the entire sequence in memory. Instead,
# it maintains a buffer in which it shuffles elements).
BUFFER_SIZE = 10000

data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

"""##make model"""

def build_model(address_predict_size, embedding_dim, rnn_units, batch_size):
  model = tf.keras.Sequential([
    tf.keras.layers.Embedding(address_predict_size, embedding_dim,
                              batch_input_shape=[batch_size, None]),
    tf.keras.layers.LSTM(rnn_units,
                        return_sequences=True,
                        stateful=True,
                        recurrent_initializer='glorot_uniform'),
    tf.keras.layers.Dense(address_predict_size)
  ])
  return model

model = build_model(ADDRESS_PREDICT_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)
model.summary()

"""##Create loss function"""

for input_example_batch, target_example_batch in data.take(1):
  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)
  print(example_batch_predictions.shape, "# (batch_size, sequence_length, address_prediect_size)")  # print out the output shape

# we can see that the predicition is an array of 64 arrays, one for each entry in the batch
print(len(example_batch_predictions))
print(example_batch_predictions)

# lets examine one prediction
pred = example_batch_predictions[0]
print(len(pred))
print(pred)
# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step

# and finally well look at a prediction at the first timestep
time_pred = pred[0]
print(len(time_pred))
print(time_pred)
# and of course its 65 values representing the probabillity of each character occuring next

# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)
sampled_indices = tf.random.categorical(pred, num_samples=1)

# now we can reshape that array and convert all the integers to numbers to see the actual characters
sampled_indices = np.reshape(sampled_indices, (1, -1))[0]
predicted = sampled_indices

predicted  # and this is what the model predicted for training sequence 1

def loss(labels, logits):
  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)

model.compile(optimizer='adam', loss=loss)

# Directory where the checkpoints will be saved
checkpoint_dir = './training_checkpoints'
# Name of the checkpoint files
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}")

checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_prefix,
    save_weights_only=True)

history = model.fit(data, epochs=10, callbacks=[checkpoint_callback])

model = build_model(ADDRESS_PREDICT_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)

model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))
model.build(tf.TensorShape([1, None]))

def generate_address(model, start_address):
  # Evaluation step (generating address using the learned model)

  # Number of characters to generate
  num_generate = 2

  input_eval = tf.expand_dims(start_address, 0)

  # Empty string to store our results
  generated = []

  # Low temperatures results in more predictable.
  temperature = 1.0

  # Here batch size == 1
  model.reset_states()
  for i in range(num_generate):
      predictions = model(input_eval)
      # remove the batch dimension
    
      predictions = tf.squeeze(predictions, 0)

      # using a categorical distribution to predict the character returned by the model
      predictions = predictions / temperature
      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()

      # We pass the predicted character as the next input to the model
      # along with the previous hidden state
      input_eval = tf.expand_dims([predicted_id], 0)

      generated.append(predicted_id)

  return generated

inp = [687, 312, 445, 755, 611, 566, 515, 584, 334, 698, 576, 584, 544, 164, 903, 564, 582, 532, 1000, 0]
print(generate_address(model, inp))

"""##accuracy 측정"""

total_num = 0
total_num_except_0_1000 = 0
correct = 0

for i in range(len(delta)-seq_length):
  total_num += 1
  if delta[i+seq_length] == 1000:
    continue
  if delta[i+seq_length] == 0:
    continue
  total_num_except_0_1000 += 1
  lstm_ans = generate_address(model,delta[i:i+seq_length])
  if delta[i+seq_length] in lstm_ans:
    correct += 1

print("accuracy")
print(correct/total_num)
print("accuracy except 0 and 1000")
print(correct/total_num_except_0_1000)

"""###seq_length = 5
#####accuracy: 3.7%
#####accuracy except 0 and 1000: 14.5%
###seq_length = 10
#####accuracy: 4.55%
#####accuracy except 0 and 1000: 17.7%
#####33min
###seq_length = 100
#####accuracy:
#####accuracy except 0 and 1000:
"""

